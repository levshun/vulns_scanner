{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import pylcs\n",
    "from typing import Union, List, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "import transformers\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "                         pipeline, TrainingArguments, Trainer,\n",
    "                         DataCollatorForTokenClassification, EarlyStoppingCallback)\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, Features, Sequence, Value, ClassLabel, load_dataset, load_from_disk\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "import re\n",
    "\n",
    "\n",
    "print(transformers.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "# model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"/home/mikhail/Documents/pandan_study/vkr/vulns_scanner/mikhail_code/models/nuner2_v1_150325/best_model_tmp\"\n",
    "final_tokenizer = AutoTokenizer.from_pretrained(path_to_model, use_fast=True, add_prefix_space=True, local_files_only=True)\n",
    "final_model = AutoModelForTokenClassification.from_pretrained(path_to_model, local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: version   Word:  5.1-BETA-1   Prob: 0.99713945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikhail/.local/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:398: UserWarning: Tokenizer does not support real words, using fallback heuristic\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "s = 'Improper Restriction of XML External Entity Reference in GitHub repository hazelcast/hazelcast in 5.1-BETA-1'\n",
    "\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=final_model, aggregation_strategy=\"first\", tokenizer=final_tokenizer\n",
    ")\n",
    "res = token_classifier(s)\n",
    "for i, r in enumerate(res):\n",
    "    # print('Entity: '+ r['entity_group'] + '   Word: ' + r['word'])\n",
    "    print('Entity: '+ r['entity_group'] + '   Word: ' + r['word'] + '   Prob: ' + str(r['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(input_string: str) -> str:\n",
    "    token_classifier = pipeline(\n",
    "        \"token-classification\", model=final_model, aggregation_strategy=\"first\", tokenizer=final_tokenizer\n",
    "    )\n",
    "    res = token_classifier(input_string)\n",
    "    output = ''\n",
    "    for i, r in enumerate(res):\n",
    "        # print('Entity: '+ r['entity_group'] + '   Word: ' + r['word'])\n",
    "        output += str(f'{i+1}. ' + 'Entity: ' + r['entity_group'] + '   Word: ' + r['word'] + '   Prob: ' + str(r['score']) + '\\n')\n",
    "    # print(output)\n",
    "    if output == '':\n",
    "        output = 'No NER found'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/home/mikhail/.local/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:398: UserWarning: Tokenizer does not support real words, using fallback heuristic\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Entity: vendor   Word:  Google   Prob: 0.9997322\\n2. Entity: version   Word:  6.1,   Prob: 0.99980503\\n3. Entity: version   Word:  5.15,   Prob: 0.9998227\\n4. Entity: version   Word:  5.10,   Prob: 0.99973506\\n5. Entity: version   Word:  5.4,   Prob: 0.9997838\\n6. Entity: version   Word:  4.19.   Prob: 0.999775\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run('Out-of-Bounds Read in ip_set_bitmap_ip.c in Google ChromeOS Kernel Versions 6.1, 5.15, 5.10, 5.4, 4.19. on All devices where Termina is used allows an attacker with CAP_NET_ADMIN privileges to cause memory corruption and potentially escalate privileges via crafted ipset commands.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
